{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist with Fully Convolutional Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data', validation_size=0, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. input: [batch_size, 28, 28, 1] images\n",
    "2. layer1: conv layer, 32 filters, kernel_size=3, padding=same, activation=relu, strides=2, max_pooling\n",
    "3. layer2: conv layer, 64 filters, kernel_size=3, padding=same, activation=relu, strides=2, max_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_inputs():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1])\n",
    "    y_ = tf.placeholder(dtype=tf.float32, shape=[None, 1, 1, 10])\n",
    "    is_train = tf.placeholder(dtype=tf.bool)\n",
    "    return x, y_, is_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(x, is_train):\n",
    "    layer1 = tf.layers.conv2d(x, filters=64, kernel_size=3, padding=\"same\", activation=tf.nn.relu)\n",
    "    layer1 = tf.layers.max_pooling2d(layer1, pool_size=2, strides=2, padding=\"valid\")\n",
    "    # now 14x14x64\n",
    "    \n",
    "    layer2 = tf.layers.conv2d(layer1, filters=128, kernel_size=5, padding=\"valid\", activation=tf.nn.relu)\n",
    "    # now 10x10x128\n",
    "    layer2 = tf.layers.max_pooling2d(layer2, pool_size=2, strides=2, padding=\"valid\")\n",
    "    # now 5x5x128\n",
    "    \n",
    "    layer3 = tf.layers.conv2d(layer2, filters=1024, kernel_size=5, padding=\"valid\", activation=tf.nn.relu)\n",
    "    # now 1x1x1024\n",
    "    \n",
    "    # dropout \n",
    "    layer4 = tf.layers.dropout(layer3, rate=0.5, training=is_train)\n",
    "    \n",
    "    logits = tf.layers.conv2d(layer4, filters=10, kernel_size=1, padding=\"valid\", activation=None)\n",
    "    # now 1x1x10\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_loss(logits, y_):\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_opt(loss, learn_rate=0.001, beta=0.9):\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learn_rate, beta1=beta).minimize(loss)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_networks(learn_rate=0.001, beta=0.9):\n",
    "    x, y_, is_train = define_inputs()\n",
    "    logits = define_model(x, is_train)\n",
    "    loss = define_loss(logits, y_)\n",
    "    opt = define_opt(loss, learn_rate=learn_rate, beta=beta)\n",
    "    return logits, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(mnist, batch_size, steps, learn_rate=0.001, beta=0.9):\n",
    "    # first, define model \n",
    "    tf.reset_default_graph()\n",
    "    x, y_, is_train=define_inputs()\n",
    "    logits = define_model(x, is_train)\n",
    "    loss = define_loss(logits, y_)\n",
    "    opt = define_opt(loss, learn_rate=learn_rate, beta=beta)\n",
    "    \n",
    "    # second, calc accuracy \n",
    "    correct_pred = tf.equal(tf.argmax(logits, 3), tf.argmax(y_, 3))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    # start training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for step in range(steps):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            imgs = batch[0].reshape(batch_size, 28, 28, 1)\n",
    "            labels = batch[1].reshape(batch_size, 1, 1, 10)\n",
    "            \n",
    "            if step%20 == 0:\n",
    "                train_acc = sess.run(accuracy, feed_dict={x: imgs, y_:labels, is_train:False})\n",
    "                print(\"step {0}, acc: {1}\".format(step, train_acc))\n",
    "            \n",
    "            sess.run(opt, feed_dict={x:imgs, y_:labels, is_train:True})\n",
    "        \n",
    "        # test model after training is finished \n",
    "        batch_size_test = 1000\n",
    "        batch_test = mnist.test.next_batch(batch_size_test)\n",
    "        imgs_test = batch_test[0].reshape(batch_size_test, 28, 28, 1)\n",
    "        labels_test = batch_test[1].reshape(batch_size_test, 1, 1, 10)\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict={x: imgs, y_:labels, is_train:False})\n",
    "        print(\"testing, acc: {0}\".format(test_acc))\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, acc: 0.03999999910593033\n",
      "step 20, acc: 0.6800000071525574\n",
      "step 40, acc: 0.8999999761581421\n",
      "step 60, acc: 0.8799999952316284\n",
      "step 80, acc: 0.9399999976158142\n",
      "step 100, acc: 0.9399999976158142\n",
      "step 120, acc: 0.9599999785423279\n",
      "step 140, acc: 1.0\n",
      "step 160, acc: 0.9599999785423279\n",
      "step 180, acc: 0.9800000190734863\n",
      "step 200, acc: 0.9800000190734863\n",
      "step 220, acc: 0.9800000190734863\n",
      "step 240, acc: 0.9800000190734863\n",
      "step 260, acc: 1.0\n",
      "step 280, acc: 0.9599999785423279\n",
      "step 300, acc: 0.9599999785423279\n",
      "step 320, acc: 1.0\n",
      "step 340, acc: 0.9800000190734863\n",
      "step 360, acc: 1.0\n",
      "step 380, acc: 0.9399999976158142\n",
      "testing, acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "train(mnist, batch_size=50, steps=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
