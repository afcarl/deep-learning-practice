{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Test\n",
    "I will build cnn with and without residual shortcut, then compare their performance. This is NOT a thorough test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use MNIST dataset for this simple test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data', validation_size=0, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define method to build cnn model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_inputs():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1])\n",
    "    y_ = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
    "    keep_prob = tf.placeholder(dtype=tf.float32, shape=())\n",
    "    return x, y_, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_block(layer_in, n_filters, kernel_size, padding=\"same\", is_residual=False, is_batch_norm=False):\n",
    "    \"\"\"\n",
    "    build a block with 2 cnn layers. Relu is used for activation function \n",
    "    \"\"\"\n",
    "    # first layer\n",
    "    layer = tf.layers.conv2d(\n",
    "        inputs=layer_in, \n",
    "        filters=n_filters, \n",
    "        kernel_size=kernel_size, \n",
    "        padding=padding, \n",
    "        activation=None)\n",
    "    if is_batch_norm:\n",
    "        layer = tf.layers.batch_normalization(layer)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    # second layer \n",
    "    layer = tf.layers.conv2d(\n",
    "        inputs=layer, \n",
    "        filters=n_filters, \n",
    "        kernel_size=kernel_size, \n",
    "        padding=padding, \n",
    "        activation=None)\n",
    "    if is_batch_norm:\n",
    "        layer = tf.layers.batch_normalization(layer)\n",
    "    if is_residual:\n",
    "        layer += layer_in\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(x, n_filters, n_blocks, keep_prob, is_residual, is_batch_norm):\n",
    "    # 28 x 28 x 1 input \n",
    "    # for simplicity, the first layer is always the same\n",
    "    layer_cur = tf.layers.conv2d(x, filters=n_filters, kernel_size=3, padding=\"same\", activation=tf.nn.relu)\n",
    "    layer_cur = tf.layers.max_pooling2d(layer_cur, pool_size=2, strides=2)\n",
    "    \n",
    "    # build multiple blocks \n",
    "    for _ in range(n_blocks):\n",
    "        layer_cur = cnn_block(\n",
    "            layer_in=layer_cur, \n",
    "            n_filters=n_filters, \n",
    "            kernel_size=3, \n",
    "            is_residual=is_residual, \n",
    "            is_batch_norm=is_batch_norm)\n",
    "    \n",
    "\n",
    "    # last pooling\n",
    "    layer_pooling = tf.layers.max_pooling2d(layer_cur, pool_size=2, strides=2)\n",
    "    # now, 7 x 7 x 8\n",
    "    \n",
    "    # for simplicity, the last layer is always the same \n",
    "    layer_flatten = tf.reshape(layer_pooling, shape=[-1, 7*7*n_filters])\n",
    "    layer_last = tf.layers.dense(layer_flatten, units=1024, activation=tf.nn.relu)\n",
    "    layer_last = tf.nn.dropout(layer_last, keep_prob=keep_prob)\n",
    "    logit = tf.layers.dense(layer_last, units=10, activation=None)\n",
    "    \n",
    "    return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(n_blocks, n_steps, is_residual, is_batch_norm=True):\n",
    "    tf.reset_default_graph()\n",
    "    x, y_, keep_prob = define_inputs()\n",
    "    logit = define_model(x, n_filters=256, n_blocks=n_blocks, keep_prob=keep_prob, \n",
    "                         is_residual=is_residual, is_batch_norm=is_batch_norm)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logit))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(logit,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(n_steps):\n",
    "            batch = mnist.train.next_batch(50)\n",
    "            imgs = batch[0].reshape(-1, 28, 28, 1)\n",
    "            labels = batch[1]\n",
    "            if i%100 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x:imgs, y_: labels, keep_prob: 1})\n",
    "                print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            train_step.run(feed_dict={x: imgs, y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "        print(\"test accuracy %g\"%accuracy.eval(\n",
    "                feed_dict={x: mnist.test.images.reshape(-1, 28, 28, 1), y_: mnist.test.labels, keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing plain CNN with 16 blocks (32 layers) + 1 cnn + 2 dense layers; thus total 35 layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08\n"
     ]
    }
   ],
   "source": [
    "train_model(n_blocks=2, n_steps=1, is_residual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Residual Network with 16 blocks (32 layers) + 1 cnn + 2 dense layers; thus total 35 layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model(n_blocks=2, n_steps=100, is_residual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
