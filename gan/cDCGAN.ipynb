{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../MNIST_data', validation_size=0, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = mnist.train.images[2]\n",
    "img = img.reshape((28, 28))\n",
    "plt.imshow(img, cmap=\"Greys_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Discriminator_Regularizer(D1_logits, D1_arg, D2_logits, D2_arg, batch_size):\n",
    "    D1 = tf.nn.sigmoid(D1_logits)\n",
    "    D2 = tf.nn.sigmoid(D2_logits)\n",
    "    grad_D1_logits = tf.gradients(D1_logits, D1_arg)[0]\n",
    "    grad_D2_logits = tf.gradients(D2_logits, D2_arg)[0]\n",
    "    grad_D1_logits_norm = tf.norm(tf.reshape(grad_D1_logits, [batch_size,-1]), axis=1, keep_dims=True)\n",
    "    grad_D2_logits_norm = tf.norm(tf.reshape(grad_D2_logits, [batch_size,-1]), axis=1, keep_dims=True)\n",
    "\n",
    "    #set keep_dims=True/False such that grad_D_logits_norm.shape == D.shape\n",
    "    print(grad_D1_logits_norm.shape)\n",
    "    print(D1.shape)\n",
    "    assert grad_D1_logits_norm.shape == D1.shape\n",
    "    assert grad_D2_logits_norm.shape == D2.shape\n",
    "\n",
    "    reg_D1 = tf.multiply(tf.square(1.0-D1), tf.square(grad_D1_logits_norm))\n",
    "    reg_D2 = tf.multiply(tf.square(D2), tf.square(grad_D2_logits_norm))\n",
    "    disc_regularizer = tf.reduce_mean(reg_D1 + reg_D2)\n",
    "    return disc_regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.2):\n",
    "    return tf.maximum(x * alpha, x)\n",
    "\n",
    "def conv2d(x, W_size, name, strides=[1, 1, 1, 1]):\n",
    "    \"\"\"\n",
    "    make conv layer\n",
    "    \n",
    "    :param x: input to conv layer. the size must match with W_size. \n",
    "    :param W_size: list. [filter_width, filter_height, input_channel, output_channel]\n",
    "    :param strides: list of strides of each dimension as follows. [batch, width, height, channel] \n",
    "    :return: returns conv layer with bias b added. \n",
    "    \"\"\"\n",
    "    W = tf.get_variable(shape=W_size, name=name+\"/weight\")\n",
    "    b = tf.get_variable(shape=W_size[3], name=name+\"/bias\")\n",
    "    conv = tf.nn.conv2d(input=x, filter=W, strides=strides, padding=\"SAME\")\n",
    "    return conv + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_inputs_label_fill_gamma_train(x_size, z_size):\n",
    "    \"\"\"\n",
    "    initialize inputs x and z. \n",
    "    \n",
    "    x is for real data, and z is for random noise for generator. \n",
    "    \n",
    "    :param z_size: size of z vector\n",
    "    :return x and z \n",
    "    \"\"\"\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[batch_size, *x_size], name=\"input_real\")\n",
    "    z = tf.placeholder(dtype=tf.float32, shape=[batch_size, z_size], name=\"input_z\")\n",
    "    y_label = tf.placeholder(dtype=tf.float32, shape=[batch_size, 10], name=\"y_label\")\n",
    "    y_fill = tf.placeholder(dtype=tf.float32, shape=[batch_size, 28, 28, 10], name=\"y_fill\")\n",
    "    gamma = tf.placeholder(dtype=tf.float32, shape=None, name=\"gamma\")\n",
    "    is_train = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    return x, z, y_label, y_fill, gamma, is_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, y_label, z_size, training, reuse=False):\n",
    "    \"\"\"\n",
    "    :param z: a vector with length z_size, where every elements of z is between -1 and 1\n",
    "    :param y_label: condition representing 0 ~ 9 one hot vector [batch_size, 1, 1, 10]\n",
    "    :param z_size: length of the z vector\n",
    "    :param reuse: boolean. whether to reuse the trained variable of weights. \n",
    "    :param training: boolean. whether it is training or not\n",
    "    :return: creates generator and return the logits and model\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        # concat z and cond \n",
    "        z_label = tf.concat([z, y_label], axis=1)\n",
    "        \n",
    "        # layer 1: fully connected layer \n",
    "        # change the length of z such that it fits to conv layer \n",
    "        W_fc1 = tf.get_variable(shape=[z_size + 10, 7*7*256], name=\"fc_weight\")\n",
    "        b_fc1 = tf.get_variable(shape=7*7*256, name=\"fc_bias\")\n",
    "        h_fc1 = tf.matmul(z_label, W_fc1) + b_fc1\n",
    "        # now, (7*7*256, )\n",
    "\n",
    "        # layer 2: reshape layer \n",
    "        reshape2 = tf.reshape(h_fc1, shape=[-1, 7, 7, 256])\n",
    "        # now, 7 x 7 x 256\n",
    "        batch_norm2 = tf.layers.batch_normalization(reshape2, training=training)\n",
    "        relu2 = leaky_relu(batch_norm2)\n",
    "\n",
    "        # layer 3: conv layer \n",
    "        conv3 = tf.layers.conv2d_transpose(relu2, 128, 5, strides=2, padding='same', name=\"conv3\")\n",
    "        # now, 14 x 14 x 128\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3, training=training)\n",
    "        relu3 = leaky_relu(batch_norm3)\n",
    "\n",
    "        # layer 4: conv layer \n",
    "        conv4 = tf.layers.conv2d_transpose(relu3, 64, 5, strides=2, padding='same', name=\"conv4\")\n",
    "        # now, 28 x 28 x 64\n",
    "        batch_norm4 = tf.layers.batch_normalization(conv4, training=training)\n",
    "        relu4 = leaky_relu(batch_norm4)\n",
    "        \n",
    "        # layer 5: conv layer \n",
    "        conv5 = conv2d(x=relu4, W_size=[5, 5, 64, 32], name=\"conv5\")\n",
    "        # now, 28 x 28 x 32\n",
    "        batch_norm5 = tf.layers.batch_normalization(conv5, training=training)\n",
    "        relu5 = leaky_relu(batch_norm5)\n",
    "        \n",
    "        # layer 5: output layer \n",
    "        logits = conv2d(x=relu5, W_size=[5, 5, 32, 1], name=\"output\")\n",
    "        model = tf.tanh(logits)\n",
    "        \n",
    "        return logits, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, y_fill, training, reuse=False):\n",
    "    \"\"\"\n",
    "    :param x: list [None, 28, 28, 1], where every element is between -1 and 1\n",
    "    :param y_fill: list [None, 28, 28, 10], representing the digit of the number\n",
    "    :param reuse: boolean. whether to reuse the trained variable of weights. \n",
    "    :param training: bool tensor. whether it is training or not\n",
    "    :return: creates discriminator and returns logits and model \n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        x_y_fill = tf.concat((x, y_fill), axis=3)\n",
    "        \n",
    "        # layer 0: conv layer \n",
    "        conv0 = conv2d(x=x_y_fill, W_size=[5, 5, 11, 32], strides=[1, 2, 2, 1], name=\"conv0\")\n",
    "        # now, 14 x 14 x 32\n",
    "        relu0 = leaky_relu(conv0)\n",
    "        \n",
    "        # layer 1: conv layer \n",
    "        conv1 = conv2d(x=relu0, W_size=[5, 5, 32, 64], strides=[1, 2, 2, 1], name=\"conv1\")\n",
    "        # now, 7 x 7 x 64\n",
    "        relu1 = leaky_relu(conv1)\n",
    "        \n",
    "        # layer 2: conv layer \n",
    "        conv2 = conv2d(x=relu1, W_size=[5, 5, 64, 128], strides=[1, 2, 2, 1], name=\"conv2\")\n",
    "        # now, 4 x 4 x 128\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2, training=training)\n",
    "        relu2 = leaky_relu(batch_norm2)\n",
    "        \n",
    "        # layer 3: reshape \n",
    "        reshape3 = tf.reshape(relu2, shape=[-1, 4*4*128])\n",
    "        \n",
    "        # layer 4: fc layer \n",
    "        W_fc4 = tf.get_variable(shape=[4*4*128, 1], name=\"fc_weight\")\n",
    "        b_fc4 = tf.get_variable(shape=[1], name=\"fc_bias\")\n",
    "        logits = tf.matmul(reshape3, W_fc4) + b_fc4\n",
    "        # now, (1, )\n",
    "        model = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_regularized(x, y_label, y_fill, z, z_size, gamma, batch_size, is_train):\n",
    "    \"\"\"\n",
    "    define the loss functions of discriminator and generator \n",
    "    \n",
    "    :param x: real image\n",
    "    :param y_label: list size [batch_size, 1, 1, 10]\n",
    "    :param y_fill: list size [batch_size, 28, 28, 10]\n",
    "    :param z: random noize z \n",
    "    :param z_size: size of random noise \n",
    "    :param train: batch norm flag \n",
    "    :return: d_loss, g_loss\n",
    "    \"\"\"\n",
    "    # generate fake images \n",
    "    g_logits, g_model = generator(z=z, y_label=y_label, z_size=z_size, training=is_train)\n",
    "    # using real and fake images, get outputs of discriminator  \n",
    "    d_real_logits, d_real_model = discriminator(x=x, y_fill=y_fill, training=is_train)\n",
    "    d_fake_logits, d_fake_model = discriminator(x=g_model, y_fill=y_fill, training=is_train, reuse=True)\n",
    "    \n",
    "    # define discriminator loss\n",
    "    d_real_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_real_logits, labels=tf.ones_like(d_real_logits)))\n",
    "    d_fake_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_fake_logits, labels=tf.zeros_like(d_fake_logits)))\n",
    "    d_loss = d_real_loss + d_fake_loss\n",
    "    \n",
    "    # regularizing \n",
    "    d_reg = Discriminator_Regularizer(d_real_logits, x, d_fake_logits, g_model, batch_size)\n",
    "    d_loss += (gamma/2.0)*d_reg\n",
    "    \n",
    "    # define generator loss\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_fake_logits, labels=tf.ones_like(d_fake_logits)))\n",
    "    \n",
    "    return d_loss, g_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizer(d_loss, g_loss, g_lr, d_lr, g_beta1, d_beta1):\n",
    "    # get variables to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    \n",
    "    # Optimize\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        d_train_opt = tf.train.AdamOptimizer(d_lr, beta1=d_beta1).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(g_lr, beta1=g_beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# define hyperparameters\n",
    "z_size = 100\n",
    "g_learning_rate = 0.0004\n",
    "d_learning_rate = 0.0004\n",
    "x_size = [28, 28, 1]\n",
    "g_beta1 = 0.8\n",
    "d_beta1 = 0.8\n",
    "batch_size = 50\n",
    "\n",
    "# initializ inputs\n",
    "x, z, y_label, y_fill, gamma, is_train = initialize_inputs_label_fill_gamma_train(x_size=x_size, z_size=z_size)\n",
    "\n",
    "# define loss function \n",
    "d_loss, g_loss = loss_regularized(x=x, y_label=y_label, y_fill=y_fill, \n",
    "                                  z=z, z_size=z_size, gamma=gamma, batch_size=batch_size, is_train=is_train)\n",
    "\n",
    "# define optimizers \n",
    "d_opt, g_opt = optimizer(d_loss, g_loss, g_lr=g_learning_rate, d_lr=d_learning_rate, g_beta1=g_beta1, d_beta1=d_beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating every variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 1200\n",
    "batch_size = 50\n",
    "\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "saver_disc = tf.train.Saver(d_vars)\n",
    "saver_gen = tf.train.Saver(g_vars)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "sample_size = 50\n",
    "losses = []\n",
    "d_count = 0\n",
    "g_count = 0\n",
    "gamma_c = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for i in range(100):\n",
    "            # get imgs, reshape and rescale to pass to discriminator\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            real_imgs = batch[0].reshape(-1, 28, 28, 1)\n",
    "            real_imgs = real_imgs*2 - 1\n",
    "\n",
    "            # generate random vector for z \n",
    "            rand_noise = np.random.normal(-1, 1, size=(batch_size, z_size))\n",
    "            # create y_label\n",
    "            labels = batch[1]\n",
    "            # create y_fill\n",
    "            y_onehot = labels.reshape([batch_size, 1, 1, 10])\n",
    "            fills = y_onehot * np.ones([batch_size, 28, 28, 10])\n",
    "            \n",
    "            # update discriminator and generator \n",
    "            sess.run(d_opt, feed_dict={x: real_imgs, z: rand_noise, y_label: labels, \n",
    "                                       y_fill: fills, gamma: 1/(e+1)**3, is_train: True})\n",
    "            sess.run(g_opt, feed_dict={x: real_imgs, z: rand_noise, y_label: labels, \n",
    "                                       y_fill: fills, gamma: 1/(e+1)**3, is_train: True})\n",
    "            \n",
    "            # save training losses \n",
    "            train_loss_d = d_loss.eval({x: real_imgs, z: rand_noise, y_label: labels, y_fill: fills, \n",
    "                                        gamma:0, is_train:False})\n",
    "            train_loss_g = g_loss.eval({z:rand_noise, y_label: labels, y_fill: fills, gamma:0, is_train: False})\n",
    "            losses.append((train_loss_d, train_loss_g))\n",
    "            \n",
    "            with open(\"./log/train_err\", 'a') as textfile:\n",
    "                textfile.write(str(train_loss_d))\n",
    "                textfile.write(\" \")\n",
    "                textfile.write(str(train_loss_g))\n",
    "                textfile.write(\"\\n\")\n",
    "        \n",
    "        write the progress\n",
    "        with open(\"./log/progress.log\", 'a') as f:\n",
    "            f.write(str(e) + \"\\n\")\n",
    "        \n",
    "        # print generated images \n",
    "        if e%3 == 0:\n",
    "            print(\"d:{0}, g:{1}\".format(d_count, g_count))\n",
    "            # generate fake images\n",
    "            sample_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
    "            g_logits, g_imgs = sess.run(generator(z, z_size, reuse=True), feed_dict={z: sample_z, gamma:0})\n",
    "            \n",
    "            # plot fake images \n",
    "            fig, axes = plt.subplots(1, 50, figsize=(100, 2))\n",
    "            for index, ax in enumerate(axes):\n",
    "                ax.imshow(g_imgs[index][:, :, 0], cmap='Greys_r')\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "            fig.tight_layout(pad=0.2)\n",
    "            plt.title(\"epoch: {0}\".format(e))\n",
    "            plt.show()\n",
    "    \n",
    "    # save the trained generator weights\n",
    "    saver.save(sess, './checkpoints/generator_cdc.ckpt')\n",
    "    saver_disc.save(sess, './checkpoints_disc/only_disc_cdc.ckpt')\n",
    "    saver_gen.save(sess, './checkpoints_gen/only_gen_cdc.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label=\"Discriminator\", alpha=0.5)\n",
    "plt.plot(losses.T[1], label=\"Generator\", alpha=0.5)\n",
    "plt.title(\"training losses\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # restore saved model \n",
    "    saver.restore(sess, './checkpoints/generator.ckpt')\n",
    "    \n",
    "    # generate 50 samples \n",
    "    sample_z = np.random.uniform(-1, 1, size=(50, z_size))\n",
    "    g_logits, g_imgs = sess.run(generator(z, z_size, reuse=True), feed_dict={z: sample_z})\n",
    "\n",
    "    # plot fake images \n",
    "    fig, axes = plt.subplots(5, 10, figsize=(20, 10))\n",
    "    index = 0\n",
    "    for row in axes:\n",
    "        for ax in row:\n",
    "            ax.imshow(g_imgs[index][:, :, 0], cmap='Greys_r')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            index += 1\n",
    "    fig.tight_layout(pad=0.2)\n",
    "    plt.title(\"epoch: {0}\".format(e))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
